{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bible Authorship\n",
    "Authors: <a href=\"mailto:razmalkau@gmail.com\">Raz Malka</a> and <a href=\"mailto:shoham39@gmail.com\">Shoham Yamin</a>\n",
    "under the supervision of <a href=\"mailto:vlvolkov@braude.ac.il\">Prof. Zeev Volkovich</a> and <a href=\"mailto:r_avros@braude.ac.il@braude.ac.il\">Dr. Renata Avros</a>.\\\n",
    "Source:</br> https://github.com/ShohamYamin/BibleAuthorship/\n",
    "\n",
    "# 6. Train Conv-BiLSTM Model - ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - General\n",
    "Let us import the required modules for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import aaib_util as util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    Bidirectional,\n",
    "    LSTM,\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we'd like to define three lists - training books, validation books and predicted books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_books_cl0 = [\"Genesis\", \"Exodus\", \"Numeri\"]\n",
    "training_books_cl1 = [\"Psalmi\", \"Jeremia\", \"Jesaia\"]\n",
    "predicted_books = [\"Deuteronomium\", \"Iob\", \"Leviticus\", \"Esra\", \"Nehemia\", \"Josua\"]\n",
    "classifications = [\"Moses\", \"Not Moses\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Data Preparation\n",
    "Well-organized data is a necessity in the way to create a working deep neural network.\\\n",
    "Here, we load the data generated in the last task <mark>Word Embedding</mark>, and prepare the input data and labels for the models.\\\n",
    "Labels are initialized deliberately to be the index of the supposed author in the authors list defined in <mark><i>aaib_util.py</mark></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(data, label):\n",
    "    return np.full(data.shape[0], label)\n",
    "\n",
    "def merge_sets(set1, set2):\n",
    "    return np.concatenate((set1, set2))\n",
    "\n",
    "# Prepare data and labels of a given set\n",
    "def prepare_set(book_list, label):\n",
    "    x_set, y_set = None, None\n",
    "    for i in range(len(book_list)):\n",
    "        fp = open(util.file_path + \"npy_elmo\\\\embedded\\\\\" + book_list[i] + \".npy\", \"rb\")\n",
    "        data = np.load(fp)\n",
    "        labels = prepare_labels(data, label)\n",
    "        x_set = data if x_set is None else merge_sets(x_set, data)\n",
    "        y_set = labels if y_set is None else merge_sets(y_set, labels)\n",
    "    return x_set, y_set\n",
    "\n",
    "x_train_cl0, y_train_cl0 = prepare_set(training_books_cl0, 0)\n",
    "x_train_cl1, y_train_cl1 = prepare_set(training_books_cl1, 1)\n",
    "\n",
    "x_train, y_train = merge_sets(x_train_cl0, x_train_cl1), merge_sets(y_train_cl0, y_train_cl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the shapes of the training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balance of positive and negative classes (%):\n",
      "{0: 422, 1: 458}\n",
      "X Shape Before: (880, 128, 1024)\n",
      "Y Shape Before: (880,)\n",
      "Shape of X before SMOTE Handling of Imbalanced Training Data: (880, 131072)\n",
      "Shape of X after SMOTE Handling of Imbalanced Training Data: (916, 131072)\n",
      "\n",
      "Balance of positive and negative classes (%):\n",
      "{0: 458, 1: 458}\n",
      "X Shape After: (916, 128, 1024)\n",
      "Y Shape After: (916,)\n"
     ]
    }
   ],
   "source": [
    "print('\\nBalance of positive and negative classes (%):')\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "print(\"X Shape Before:\", x_train.shape)\n",
    "print(\"Y Shape Before:\", y_train.shape)\n",
    "\n",
    "orig_shape = x_train.shape\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "\n",
    "sm = SMOTE(n_jobs=6)\n",
    "\n",
    "x_sm, y_sm = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "print(f'''Shape of X before SMOTE Handling of Imbalanced Training Data: {x_train.shape}\n",
    "Shape of X after SMOTE Handling of Imbalanced Training Data: {x_sm.shape}''')\n",
    "\n",
    "print('\\nBalance of positive and negative classes (%):')\n",
    "unique, counts = np.unique(y_sm, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "x_train = np.reshape(x_sm, ((int)(x_sm.size/(orig_shape[1]*orig_shape[2])), orig_shape[1], orig_shape[2]))\n",
    "y_train = y_sm\n",
    "print(\"X Shape After:\", x_train.shape)\n",
    "print(\"Y Shape After:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - Turn Scalar Targets into Binary Categories\n",
    "Our classification model has multiple classes, and we want them distributed in a binary matrix.\\\n",
    "We use keras' <mark>to_categorical</mark> utility method to transform our label data before passing it to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn our scalar targets into binary categories\n",
    "num_classes = len(classifications)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Predictions\n",
    "Let us define two functions which will serve us for generating and interpreting predictions from a given model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_confidence(model, book):\n",
    "    prediction_matrix = model.predict((book))\n",
    "    prediction_vector = np.mean(prediction_matrix, axis=0 )\n",
    "    \n",
    "    prediction = np.argmax(prediction_vector)\n",
    "    confidence = util.truncate(prediction_vector[prediction]*100, 3)\n",
    "    return prediction, confidence\n",
    "\n",
    "def predict_style_similarity(model):\n",
    "    for b in predicted_books:\n",
    "        fp = open(util.file_path + \"npy_elmo\\\\embedded\\\\\" + b + \".npy\", \"rb\")\n",
    "        book = np.load(fp)\n",
    "        prediction, confidence = predict_with_confidence(model, book)\n",
    "        print(b, \"is determined to be\", classifications[prediction], \"with confidence of\", confidence, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 - Hybrid Model\n",
    "Let us define a Bi-Directional Long Short-Term Memory model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_hybrid_model(filters=[500,500,500], kernel_size=[3,6,12]):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters[0], kernel_size=kernel_size[0], padding=\"same\", activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(MaxPooling1D(1, padding=\"same\"))\n",
    "    model.add(Conv1D(filters=filters[1], kernel_size=kernel_size[1], padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(1, padding=\"same\"))\n",
    "    model.add(Bidirectional(LSTM(units=75, return_sequences=True), merge_mode='concat'))\n",
    "    model.add(Bidirectional(LSTM(units=75)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    return model\n",
    "defined_bilstm_model = define_hybrid_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 128, 500)          1536500   \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 128, 500)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 128, 500)          1500500   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 128, 500)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128, 150)          345600    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 150)               135600    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 302       \n",
      "=================================================================\n",
      "Total params: 3,518,502\n",
      "Trainable params: 3,518,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "defined_bilstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model's structure is defined, but it is not trained yet. It should now be compiled and fitted over the training and validation data.\\\n",
    "Below is our training loop, which will run for <mark>N_iter</mark> iterations that maintain an <mark>accuracy threshold</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Iteration 1/1 with Accuracy of 97.27%\n",
      "Achieved Accuracy: 97.2707450389862 %\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "current_iter = 0\n",
    "N_iter = 1\n",
    "accuracy = 0\n",
    "accuracy_threshold = 0.92\n",
    "top_bilstm_model = None\n",
    "top_bilstm_model_accuracy = 0\n",
    "\n",
    "# Training loop\n",
    "while current_iter < N_iter:\n",
    "    model = models.clone_model(defined_bilstm_model)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['acc'])\n",
    "    %time history = model.fit(x_train, y_train, epochs=10, verbose=1, validation_split=0.15, batch_size=50)\n",
    "    train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=0)\n",
    "    \n",
    "    if train_accuracy > top_bilstm_model_accuracy:\n",
    "        top_bilstm_model = model\n",
    "        top_bilstm_model_accuracy = train_accuracy\n",
    "        \n",
    "    if train_accuracy > accuracy_threshold:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Completed Iteration {}/{} with Accuracy of {}%\".format(current_iter + 1, N_iter, util.truncate(train_accuracy*100,3)))\n",
    "        current_iter = current_iter + 1\n",
    "    else:\n",
    "        print(\"Failed Iteration {}/{} with Accuracy of {}%\".format(current_iter + 1, N_iter, util.truncate(train_accuracy*100,3)))\n",
    "\n",
    "print(\"Achieved Accuracy:\", top_bilstm_model_accuracy*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 - Hybrid Predictions and Saving\n",
    "Now that our Conv-BiLSTM model is trained, we should check it out and let it predict supposed authorship of different books.\n",
    "Also, when having a well-trained model, we will want to deploy it to perform inference on new texts.\\\n",
    "Once we have a trained model that we are happy with, it should be saved to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deuteronomium is determined to be Not Moses with confidence of 87.427 %\n",
      "Iob is determined to be Not Moses with confidence of 96.213 %\n",
      "Leviticus is determined to be Moses with confidence of 90.824 %\n",
      "Esra is determined to be Not Moses with confidence of 92.388 %\n",
      "Nehemia is determined to be Not Moses with confidence of 92.39 %\n",
      "Josua is determined to be Not Moses with confidence of 91.09 %\n"
     ]
    }
   ],
   "source": [
    "predict_style_similarity(top_bilstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\bible\\convbilstm_model_elmo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\bible\\convbilstm_model_elmo\\assets\n"
     ]
    }
   ],
   "source": [
    "top_bilstm_model.save('models\\\\bible\\\\convbilstm_model_elmo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus - Loading a Model\n",
    "As the models were saved to disk, they can be loaded back into memory as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bilstm_model = models.load_model('models\\\\bible\\\\convbilstm_model_elmo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluated as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bilstm_model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
