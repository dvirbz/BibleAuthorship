{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bible Authorship\n",
    "Authors: <a href=\"mailto:razmalkau@gmail.com\">Raz Malka</a> and <a href=\"mailto:shoham39@gmail.com\">Shoham Yamin</a>\n",
    "under the supervision of <a href=\"mailto:vlvolkov@braude.ac.il\">Prof. Zeev Volkovich</a> and <a href=\"mailto:r_avros@braude.ac.il@braude.ac.il\">Dr. Renata Avros</a>.\\\n",
    "Source:</br> https://github.com/ShohamYamin/BibleAuthorship/\n",
    "\n",
    "# 2. Preprocessing and Dividing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - General\n",
    "In the previous task <mark>Data Preparation</mark> we prepared 39 books in plain text format, which we now wish to preprocess and divide.\\\n",
    "The preprocessing task demands the following steps:\n",
    "\n",
    "1. Remove punctuation marks and diacritics\n",
    "2. Tokenization\n",
    "\n",
    "The dividing task demands the division of tokenized books into chunks (or rather, tweets) of predefined size.\\\n",
    "\\\n",
    "Let us import the required modules for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import aaib_util as util\n",
    "import json\n",
    "import numpy as np\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Load Texts from Files\n",
    "Then, we move on to the loading task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data = []\n",
    "for i in range(len(util.books)):\n",
    "    book_data.append(open(util.file_path + 'txt\\\\' + util.books[i] + '.txt', 'r', encoding = 'utf_16_le').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Remove Punctuation Marks and Diacritics\n",
    "Before we tokenize our text, we have to clean it from excess characters and symbols.\\\n",
    "The marks <mark><b>, . : ;</b></mark> should be removed, as well as diacritics (niqqud and cantillation, which are Hebrew accents) and rogue single letters.\\\n",
    "To this task we chose the <mark><i>unicodedata</i></mark> module, which helps us normalize accented characters into their unaccented form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cantillation(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def remove_punctuation_and_niqqud(text):\n",
    "    text = text.replace('\\u05be', ' ') # Special Case for Hebrew Hyphen\n",
    "    return ''.join(['' if  (1456 <= ord(c) <= 1479 or ord(c) in [44, 46, 58, 59]) else c for c in text])\n",
    "\n",
    "def remove_single_letters(text):\n",
    "    for i in range(27):                # Hebrew has 27 letters\n",
    "        text = text.replace(' ' + chr(ord('א') + i) + ' ', ' ')\n",
    "    return text\n",
    "\n",
    "for i in range(len(util.books)):\n",
    "    book_data[i] = remove_punctuation_and_niqqud(book_data[i])\n",
    "    book_data[i] = remove_cantillation(book_data[i])\n",
    "    book_data[i] = remove_single_letters(book_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Tokenization\n",
    "Tokenization is the task of breaking a text into tokens, in our case - words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_books = []\n",
    "for i in range(len(util.books)):\n",
    "    tokenized_books.append(book_data[i].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Dividing\n",
    "The dividing task demands the division of tokenized books into chunks of predefined size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "divided_books = []\n",
    "\n",
    "def pad_last_element(l, n):\n",
    "    if (len(l[-1]) < n):\n",
    "        l[-1].extend(['' for i in range((n - len(l[-1])))])\n",
    "    return l\n",
    "\n",
    "def chunks(l, n):\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]\n",
    "\n",
    "for i in range(len(util.books)):\n",
    "    divided_books.append(chunks(tokenized_books[i], chunk_size))\n",
    "    divided_books[i] = pad_last_element(divided_books[i], chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 - Saving\n",
    "The <mark><i>JSON</i></mark> module let us save the preprocessed and divided books into files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(util.books)):\n",
    "    with open(util.file_path + \"json\\\\\" + util.books[i] + \".json\", \"w\") as fp:\n",
    "        json.dump(divided_books[i], fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra - Future Loading and Book Shapes\n",
    "Naturally, we would use those files in the future. The way to load them back here is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(util.books)):\n",
    "    with open(util.file_path + \"json\\\\\" + util.books[i] + \".json\", \"r\") as fp:\n",
    "            book = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
